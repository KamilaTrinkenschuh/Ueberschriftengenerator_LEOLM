# -*- coding: utf-8 -*-
"""newRepo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XiFfkUCjUe90p-zJatvoB44E6fa6uz9_
"""

!pip install huggingface
from huggingface_hub import notebook_login
notebook_login()
!huggingface-cli login

! pip install bitsandbytes transformers peft accelerate
! pip install datasets trl ninja packaging
# Uncomment only if you're using A100 GPU
!pip install flash-attn --no-build-isolation

import torch
import os
import sys
import json
import IPython
from datetime import datetime
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments,
)
from trl import SFTTrainer

# Chose the base model you want
model_name = "LeoLM/leo-hessianai-7b"
# set device
device = 'cuda'
#v Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)
tokenizer.pad_token = tokenizer.unk_token
tokenizer.pad_token_id = tokenizer.unk_token_id

compute_dtype = getattr(torch, "float16") #-> gives one the data type torch.float16
print(compute_dtype)
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=True,
)
!pip install -i https://pypi.org/simple/ bitsandbytes
!pip install accelerate

#Load the model and quantize it
model = AutoModelForCausalLM.from_pretrained(
          model_name,
          quantization_config=bnb_config,
          use_flash_attention_2 = True, #set to True you're using A100
          device_map={"": 0}, #device_map="auto" will cause a problem in the training

)
model

model1 = AutoModelForCausalLM.from_pretrained(model_name)

import pandas as pd
from datasets import load_dataset

df = load_dataset("bjoernp/tagesschau-2018-2023")

# Convert to pandas DataFrame for convenient processing
df1 = pd.DataFrame(df['train'])

# Combine the two attributes into an instruction string
df1['instruction'] = 'Füge dem folgenden kurzen Text eine Überschrift zu: ' + df1['article']
df1 = df1[['instruction', 'headline']]

# Get a 5000 sample subset for fine-tuning purposes
rd_df_sample = df1.sample(n=5000, random_state=42)

# Define template and format data into the template for supervised fine-tuning
template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:

{}

### Response:\n"""

rd_df_sample['prompt'] = rd_df_sample["instruction"].apply(lambda x: template.format(x))

# Renaming the 'article' column to 'response'
rd_df_sample.rename(columns={'headline': 'response'}, inplace=True)
rd_df_sample['response'] = rd_df_sample['response'] + "\n### End"
rd_df_sample = rd_df_sample[['prompt', 'response']]

# Correctly creating the 'text' column in df1
df1['text'] = df1["instruction"] + df1["headline"]
df1.drop(columns=['instruction', 'headline'], inplace=True)

import torch
from transformers import LlamaTokenizer, LlamaForCausalLM
prompt = 'Q: Verfasse für den folgenden Text eine Überschrift: Etwa 1,5 Millionen türkische Staatsbürger in Deutschland können von heute an ihre Stimme für die Parlaments- und Präsidentenwahlen in der Türkei abgeben. Präsident Erdogan sagte wegen gesundheitlicher Probleme weitere Wahlkampftermine ab. mehr\nA:'
input_ids = tokenizer(prompt, return_tensors="pt").input_ids
input_ids = input_ids.to('cuda')

generation_output = model.generate(
    input_ids=input_ids,
    max_length=300,
    num_return_sequences=1,
    temperature=1.0,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.2,
    do_sample=True,
    pad_token_id=tokenizer.pad_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)

generated_text = tokenizer.decode(generation_output[0], skip_special_tokens=True)
print(generated_text)

prompt= """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
 Verfasse für den folgenden Text eine Überschrift: Etwa 1,5 Millionen türkische Staatsbürger in Deutschland können von heute an ihre Stimme für die Parlaments- und Präsidentenwahlen in der Türkei abgeben. Präsident Erdogan sagte wegen gesundheitlicher Probleme weitere Wahlkampftermine ab.

### Response:"""
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

outputs = model1.generate(input_ids, do_sample=False, max_length=200)
tokenizer.batch_decode(outputs, skip_special_tokens=True)



peft_config = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.05,
        r=16,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', "gate_proj", "down_proj", "up_proj", "lm_head",]
)

#Cast some modules of the model to fp32
model = prepare_model_for_kbit_training(model)
#Configure the pad token in the model
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching

training_arguments = TrainingArguments(
        output_dir="./results", # directory in which the checkpoint will be saved.
        evaluation_strategy="epoch", # you can set it to 'steps' to eval it every eval_steps
        optim="paged_adamw_8bit", #used with QLoRA
        per_device_train_batch_size=4, #batch size
        per_device_eval_batch_size=4, #same but for evaluation
        gradient_accumulation_steps=1, #number of lines to accumulate gradient, carefull because it changes the size of a "step".Therefore, logging, evaluation, save will be conducted every gradient_accumulation_steps * xxx_step training example
        log_level="debug", #you can set it to  ‘info’, ‘warning’, ‘error’ and ‘critical’
        save_steps=500, #number of steps between checkpoints
        logging_steps=20, #number of steps between logging of the loss for monitoring adapt it to your dataset size
        learning_rate=4e-5, #you can try different value for this hyperparameter
        num_train_epochs=1,
        warmup_steps=100,
        lr_scheduler_type="constant",
)

import re
model_modules = str(model.modules)
pattern = r'\((\w+)\): Linear'
linear_layer_names = re.findall(pattern, model_modules)

names = []
# Print the names of the Linear layers
for name in linear_layer_names:
    names.append(name)
target_modules = list(set(names))

rd_df_sample.keys()
rd_df_sample

from datasets import *
from trl.trainer import SFTTrainer
from sklearn.model_selection import train_test_split

# Assuming rd_df_sample is your DataFrame
train_df, test_df = train_test_split(rd_df_sample, test_size=0.3, shuffle=True)

# Now you can use the SFTTrainer with the dataset objects
train_df, test_df

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Let's print the column names to see the available columns in the datasets
train_dataset.column_names
test_dataset.column_names

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    peft_config=peft_config,
    dataset_text_field="response",
    tokenizer=tokenizer,
    args=training_arguments,
)
print(trainer)

trainable_params = 0
all_param = model.num_parameters()
for _, param in model.named_parameters():
  if param.requires_grad:
    trainable_params += param.numel()

    print(f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}")

trainer.evaluate()
# Launch the training
trainer.train()
from google.colab import drive
drive.mount('/content/gdrive')

new_model = 'LeoLMfinetuning1_Tagesschau1'
newmodel = trainer.model.save_pretrained(new_model)
base_model = AutoModelForCausalLM.from_pretrained(model_name)
peft_model = PeftModel.from_pretrained(base_model, new_model)
merged_model = peft_model.merge_and_unload()
output_merged_dir = "/content/LEO_german_finetuned"

newmodel = trainer.model.save_pretrained(new_model)

#trainer.evaluate()
eval_prompt = """Instruction: Vefasse eine Überschrift für den folgenden Text: Das Geschäft ist besiegelt: Der Heizungsbauer Viessmann verkauft seine Klimatechniksparte für zwölf Milliarden Euro an den US-Konzern Carrier Global. Wirtschaftsminister Habeck will die Übernahme prüfen. Der hessische Heizungsbauer Viessmann verkauft seine Klimasparte einschließlich der lukrativen Wärmepumpen an den US-Konkurrenten Carrier Global. Dieser bezifferte den Preis auf zwölf Milliarden Euro. Die verbleibende Viessmann-Gruppe erhält 80 Prozent des Kaufpreises in bar, die restlichen 20 Prozent als Aktienpaket. Dadurch wird die Viessmann-Gruppe einer der größten Anteilseigner des US-Konzerns. Das Geschäft soll bis zum Ende des Jahres abgeschlossen sein. Der Kaufpreis entspreche dem 13-fachen des für 2023 erwarteten operativen Ergebnisses (Ebitda), teilte Carrier in der Nacht auf Mittwoch mit. Langfristige Garantien für Mitarbeiter Beide Seiten hätten sich auf langfristige Garantien geeinigt, teilte Viessmann mit. So seien betriebsbedingte Kündigungen für drei Jahre ausgeschlossen, wichtige Standorte für fünf Jahre gesichert und Allendorf an der Eder für zehn Jahre als Hauptsitz gesetzt. An die Mitarbeiter der Sparte sollen 106 Millionen Euro als Sonderprämie "für 106 Erfolgsjahre" ausgeschüttet werden. Carrier setzt auf Siegeszug der Wärmepumpe Mit dem Verkauf entstehe ein "zukunftssicherer globaler Klima-Champion", erklärte Konzernchef Max Viessmann, der in den Verwaltungsrat von Carrier einzieht. "Wir können die weltweite Energiewende nur dann erfolgreich meistern, wenn Unternehmen global denken, handeln und zusammenarbeiten." Carrier-Chef David Gittin bezeichnete die Akquisition als "spielverändernde Gelegenheit". Die Viessmann-Klimasparte mit 11.000 Beschäftigten sei entscheidend für die europäische Energiewende. Carrier setzt mit der Übernahme vor allem auf den Siegeszug der Wärmepumpe: Der Markt in Europa werde sich bis 2027 auf 15 Milliarden Euro verdreifachen. Guter Marktzugang über Installateure Dabei will das US-Unternehmen künftig auch vom Marktzugang über 75.000 Installateure in 25 Ländern profitieren, die Viessmann-Produkte in die Haushalte bringen könnten. Das ist ein großer Vorteil gegenüber den asiatischen Anbietern, die in der Massenproduktion von Klimaanlagen führend sind, welche mit Wärmepumpen in weiten Teilen bauähnlich sind. Bekannte asiatische Anbieter sind Daikin, Mitsubishi (beide Japan), Midea (China) oder Samsung (Korea). Doch etwa in Deutschland fehlt ihnen bislang noch der Marktzugang über die Installateure. Zwei Unternehmen mit langer Tradition Viessmann ist neben Bosch (Buderus) und Vaillant einer der größten Heizungshersteller in Deutschland. Der Geschäftsbereich Klimalösungen steht für 85 Prozent der Umsätze, die 2022 auf den Rekordwert von rund vier Milliarden Euro angestiegen waren. Das 1917 aus einer Schlosserei gegründete Unternehmen gehört zu den bekanntesten deutschen Heizungsbauern und zählte bislang zu den Gewinnern der Klimawende insbesondere im Gebäudebereich. Das Unternehmen Carrier aus dem US-Staat Florida gilt als Erfinder der modernen Klimaanlage und wurde 1902 gegründet. Der Konzern beschäftigt 52.000 Menschen und erlöste im vergangenen Jahr 20,4 Milliarden Dollar. 60 Prozent des Umsatzes entfielen auf Nord- und Südamerika. Deal nicht unumstritten Das Geschäft zwischen Viessmann und Carrier wird von Politikern und Ökonomen hierzulande nicht nur positiv gesehen. Einige kritische Stimmen warnen, dass Deutschland nach dem Niedergang der Solarenergiebranche nun die nächste Zukunftstechnologie zu verlieren drohe. Bundeswirtschaftsminister Robert Habeck will den milliardenschweren Verkauf unter die Lupe nehmen. "Wir werden uns das Vorhaben im Rahmen der vorgesehenen Prüfschritte anschauen und sind im Gespräch mit dem Verkäufer und dem Investor, damit das Projekt unserer Wirtschaft und dem Standort Deutschland dient", erklärte der Grünen-Politiker. Wichtig sei, "dass die Vorteile unserer Energiepolitik und Gewinne, die damit erwirtschaftet werden, auch weiter dem Standort Deutschland zugutekommen". Darauf werde die Regierung achten."""

# import random
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

model.eval()
with torch.no_grad():
    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))
model.train()

from tensorboard import notebook
log_dir = "results/runs"
notebook.start("--logdir {} --port 4000".format(log_dir))

import locale
locale.getpreferredencoding = lambda: "UTF-8"
!pip install huggingface_hub

from huggingface_hub import notebook_login
notebook_login()

model.push_to_hub("Kamilatr/Ueberschriftengenerator_LEOLM")